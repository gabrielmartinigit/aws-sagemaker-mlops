{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    CategoricalParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterBoolean,\n",
    ")\n",
    "from sagemaker.processing import Processor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo, ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "bucket = 'cgu-poc-sagemaker'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.4xlarge\"\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "training_start = ParameterBoolean(\n",
    "    name=\"TrainingStart\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.p3.8xlarge\"\n",
    ")\n",
    "\n",
    "training_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "min_accuracy = ParameterFloat(\n",
    "    name=\"Accuracy\",\n",
    "    default_value=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_container_uri = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.18.0\"\n",
    "flow_volume_size_in_gb = 100\n",
    "database = \"cgu-poc-analytics\"\n",
    "table = \"analytics_feedbacks\"\n",
    "output_name = \"64859aa2-7440-486e-abf8-e1f23f9d9314.default\"\n",
    "output_config = {\n",
    "    output_name:{}\n",
    "}\n",
    "\n",
    "## Input - Flow: data-wrangler-feedbacks.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source = \"./data-wrangler-feedbacks.flow\",\n",
    "    destination = \"/opt/ml/processing/flow\",\n",
    "    input_name = \"flow\",\n",
    "    s3_data_type = \"S3Prefix\",\n",
    "    s3_input_mode = \"File\",\n",
    "    s3_data_distribution_type = \"FullyReplicated\"\n",
    ")\n",
    "\n",
    "athena_dataset_definition = AthenaDatasetDefinition(\n",
    "    catalog = \"AwsDataCatalog\",\n",
    "    database = database,\n",
    "    query_string = f\"SELECT * FROM {table}\",\n",
    "    output_format = \"PARQUET\",\n",
    "    output_s3_uri = f\"s3://{bucket}/processing\",\n",
    ")\n",
    "\n",
    "dataset_definition = DatasetDefinition(\n",
    "    athena_dataset_definition = athena_dataset_definition,\n",
    "    local_path = \"/opt/ml/processing/output\"\n",
    ")\n",
    "\n",
    "athena_input = ProcessingInput(\n",
    "    destination = \"/opt/ml/processing/output\",\n",
    "    input_name = \"americanas-dataset\",\n",
    "    dataset_definition = dataset_definition\n",
    ")\n",
    "\n",
    "flow_output = ProcessingOutput(\n",
    "    output_name = output_name,\n",
    "    source = f\"/opt/ml/processing/output/{output_name}\",\n",
    "    destination = f\"s3://{bucket}/datasets/\",\n",
    "    s3_upload_mode = \"EndOfJob\"\n",
    ")\n",
    "\n",
    "data_wrangler_processor = Processor(\n",
    "    role = role,\n",
    "    image_uri = flow_container_uri,\n",
    "    instance_count = processing_instance_count,\n",
    "    instance_type = processing_instance_type,\n",
    "    volume_size_in_gb = flow_volume_size_in_gb,\n",
    ")\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    name = \"DataWranglerProcessingStep\",\n",
    "    processor = data_wrangler_processor,\n",
    "    inputs = [\n",
    "        flow_input,\n",
    "        athena_input\n",
    "    ], \n",
    "    outputs = [\n",
    "        flow_output\n",
    "    ],\n",
    "    job_arguments = [f\"--output-config '{json.dumps(output_config)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'models/feedbacks'\n",
    "output_path = f\"s3://{bucket}/{model_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"../modeltrain/script\",\n",
    "    role=role,\n",
    "    framework_version=\"1.10.2\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=training_instance_count,\n",
    "    instance_type=training_instance_type,\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"batch-size\": 16,\n",
    "        \"epochs\": 1,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    },\n",
    "    disable_profiler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"lr\": CategoricalParameter([3e-4, 1e-4, 5e-5, 3e-5]),\n",
    "    \"batch-size\": CategoricalParameter([4, 8, 16]),\n",
    "}\n",
    "\n",
    "# change to accuracy\n",
    "objective_metric_name = \"accuracy\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": \"accuracy\", \"Regex\": \"=====>#011{'accuracy': ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TuningStep(\n",
    "    name=\"BERTTrain\",\n",
    "    tuner=tuner,\n",
    "    inputs={\n",
    "        \"training\": TrainingInput(\n",
    "            s3_data=data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri\n",
    "            # s3_data=\"s3://cgu-poc-sagemaker/datasets/data-wrangler-feedbacks-2022-06-14T03-02-28/train/feedbacks_train.csv\"\n",
    "        ),\n",
    "        \"testing\": TrainingInput(\n",
    "            # s3_data=data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri\n",
    "            s3_data=\"s3://cgu-poc-sagemaker/datasets/data-wrangler-feedbacks-2022-06-14T03-02-28/test/feedbacks_test.csv\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "step_train.add_depends_on([data_wrangler_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_register = RegisterModel(\n",
    "    name=\"BERTRegisterModel\",\n",
    "    estimator=estimator,\n",
    "    content_types=[\"application/json\"],\n",
    "    model_data=step_train.get_top_model_s3_uri(top_k=0, s3_bucket=f\"{bucket}/models/feedbacks\"),\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[training_instance_type],\n",
    "    transform_instances=[training_instance_type],\n",
    "    model_package_group_name=\"feedbacks-model-group\",\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_name = \"feedbacks-pipeline\"\n",
    "\n",
    "# Combine pipeline steps and create pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        model_approval_status,\n",
    "        min_accuracy,\n",
    "        training_start\n",
    "    ],\n",
    "    steps=[\n",
    "        data_wrangler_step,\n",
    "        step_train,\n",
    "        step_register\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pipeline.delete()\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
