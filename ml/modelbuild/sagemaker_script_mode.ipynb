{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow==2 in /opt/conda/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: awswrangler in /opt/conda/lib/python3.8/site-packages (2.15.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.8/site-packages (from pyarrow==2) (1.22.2)\n",
      "Requirement already satisfied: requests-aws4auth<2.0.0,>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.1.2)\n",
      "Requirement already satisfied: backoff<2.0.0,>=1.11.1 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.11.1)\n",
      "Requirement already satisfied: redshift-connector<2.1.0,>=2.0.889 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (2.0.907)\n",
      "Requirement already satisfied: gremlinpython<4.0.0,>=3.5.2 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (3.6.0)\n",
      "Requirement already satisfied: openpyxl<3.1.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (3.0.10)\n",
      "Requirement already satisfied: opensearch-py<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.1.0)\n",
      "Requirement already satisfied: pg8000<2.0.0,>=1.20.0 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.29.1)\n",
      "Requirement already satisfied: pymysql<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.0.2)\n",
      "Requirement already satisfied: progressbar2<5.0.0,>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (4.0.0)\n",
      "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.5.3 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.5.3)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.4.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.20.17 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.21.13)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.23.17 in /opt/conda/lib/python3.8/site-packages (from awswrangler) (1.24.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0.0,>=1.20.17->awswrangler) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0.0,>=1.20.17->awswrangler) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<2.0.0,>=1.23.17->awswrangler) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<2.0.0,>=1.23.17->awswrangler) (1.26.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.5.5)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (0.6.1)\n",
      "Requirement already satisfied: aiohttp<=3.8.1,>=3.8.0 in /opt/conda/lib/python3.8/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (3.8.1)\n",
      "Requirement already satisfied: aenum<4.0.0,>=1.4.5 in /opt/conda/lib/python3.8/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler) (3.1.11)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (1.16.0)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.8/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (3.11)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (5.1.1)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.8/site-packages (from openpyxl<3.1.0,>=3.0.0->awswrangler) (1.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from opensearch-py<2.0.0,>=1.0.0->awswrangler) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.0->awswrangler) (2021.3)\n",
      "Requirement already satisfied: scramp>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from pg8000<2.0.0,>=1.20.0->awswrangler) (1.4.1)\n",
      "Requirement already satisfied: python-utils>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from progressbar2<5.0.0,>=4.0.0->awswrangler) (3.3.3)\n",
      "Requirement already satisfied: requests<2.27.2,>=2.23.0 in /opt/conda/lib/python3.8/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /opt/conda/lib/python3.8/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (4.11.1)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /opt/conda/lib/python3.8/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (4.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from redshift-connector<2.1.0,>=2.0.889->awswrangler) (21.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (20.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler) (1.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<2.27.2,>=2.23.0->redshift-connector<2.1.0,>=2.0.889->awswrangler) (3.3)\n",
      "Requirement already satisfied: asn1crypto>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from scramp>=1.4.1->pg8000<2.0.0,>=1.20.0->awswrangler) (1.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->redshift-connector<2.1.0,>=2.0.889->awswrangler) (3.0.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==2 awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    CategoricalParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'cgu-poc-sagemaker'\n",
    "data_prefix = 'datasets/data-wrangler-feedbacks-2022-06-14T03-02-28'\n",
    "model_prefix = 'models/feedbacks'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>minha crítica não é em relação ao produto em si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>recomendo a todos. a bomba submersa anauger é ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>o cabo é ridículamente curto. a chaleira tem q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>gostaria de saber se o colchão vem incluso? po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>não vale a pena. economia porca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                           comments\n",
       "0       0    minha crítica não é em relação ao produto em si\n",
       "1       1  recomendo a todos. a bomba submersa anauger é ...\n",
       "2       0  o cabo é ridículamente curto. a chaleira tem q...\n",
       "3       0  gostaria de saber se o colchão vem incluso? po...\n",
       "4       0                    não vale a pena. economia porca"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wr.s3.read_csv(f\"s3://{bucket}/{data_prefix}/\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df)\n",
    "df.to_csv('./data/dataset.csv', index=False)\n",
    "train.to_csv(\"./data/feedbacks_train.csv\", index=False)\n",
    "test.to_csv(\"./data/feedbacks_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"./data/feedbacks_train.csv\", bucket=bucket, key_prefix=f'{data_prefix}/train')\n",
    "inputs_test = sagemaker_session.upload_data(\"./data/feedbacks_test.csv\", bucket=bucket, key_prefix=f'{data_prefix}/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-06-27-01-29-38-732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-27 01:29:38 Starting - Starting the training job......\n",
      "2022-06-27 01:30:33 Starting - Preparing the instances for training.........\n",
      "2022-06-27 01:32:07 Downloading - Downloading input data\n",
      "2022-06-27 01:32:07 Training - Downloading the training image...........................\n",
      "2022-06-27 01:36:24 Training - Training image download completed. Training in progress.\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2022-06-27 01:36:27,285 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2022-06-27 01:36:27,325 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2022-06-27 01:36:27,334 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2022-06-27 01:36:27,780 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-27 01:36:27,137 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-27 01:36:27,177 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-27 01:36:27,183 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-27 01:36:27,629 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sklearn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sklearn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.26.0)\u001b[0m\n",
      "\u001b[35mCollecting regex\u001b[0m\n",
      "\u001b[35mDownloading regex-2022.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35mCollecting sacremoses\u001b[0m\n",
      "\u001b[35mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting transformers\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from sklearn->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (4.61.2)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from sklearn->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 5)) (4.61.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (21.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (1.21.2)\u001b[0m\n",
      "\u001b[35mCollecting filelock\u001b[0m\n",
      "\u001b[35mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 6)) (1.21.2)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r requirements.txt (line 6)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 6)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r requirements.txt (line 6)) (3.10.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 6)) (3.0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 1)) (2.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[35mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=cae6514ec41e7560cf06205e33f17e9b09cd28cda2cb17b92a7b3c1e8ae881b7\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[35mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=cae6514ec41e7560cf06205e33f17e9b09cd28cda2cb17b92a7b3c1e8ae881b7\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: filelock, tokenizers, regex, huggingface-hub, transformers, sentencepiece, sacremoses\u001b[0m\n",
      "\u001b[35mInstalling collected packages: filelock, tokenizers, regex, huggingface-hub, transformers, sentencepiece, sacremoses\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.7.1 huggingface-hub-0.8.1 regex-2022.6.2 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.20.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mSuccessfully installed filelock-3.7.1 huggingface-hub-0.8.1 regex-2022.6.2 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.20.1\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-06-27 01:36:37,645 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"batch-size\": 16,\n",
      "        \"epochs\": 3,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2022-06-27-01-29-38-732\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://cgu-poc-sagemaker/pytorch-training-2022-06-27-01-29-38-732/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"batch-size\":16,\"epochs\":3,\"num_labels\":2}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://cgu-poc-sagemaker/pytorch-training-2022-06-27-01-29-38-732/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":16,\"epochs\":3,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2022-06-27-01-29-38-732\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://cgu-poc-sagemaker/pytorch-training-2022-06-27-01-29-38-732/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"16\",\"--epochs\",\"3\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 train.py --backend gloo --batch-size 16 --epochs 3 --num_labels 2\u001b[0m\n",
      "\u001b[34m2022-06-27 01:36:37,225 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"batch-size\": 16,\n",
      "        \"epochs\": 3,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-06-27-01-29-38-732\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://cgu-poc-sagemaker/pytorch-training-2022-06-27-01-29-38-732/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.8xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.8xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"batch-size\":16,\"epochs\":3,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://cgu-poc-sagemaker/pytorch-training-2022-06-27-01-29-38-732/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":16,\"epochs\":3,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-06-27-01-29-38-732\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://cgu-poc-sagemaker/pytorch-training-2022-06-27-01-29-38-732/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.8xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.8xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"16\",\"--epochs\",\"3\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --backend gloo --batch-size 16 --epochs 3 --num_labels 2\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/205k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 205k/205k [00:00<00:00, 38.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 3.29kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/112 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 112/112 [00:00<00:00, 170kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/205k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 205k/205k [00:00<00:00, 44.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 3.33kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/112 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 112/112 [00:00<00:00, 175kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 43.0/43.0 [00:00<00:00, 62.3kB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/647 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 647/647 [00:00<00:00, 1.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 43.0/43.0 [00:00<00:00, 70.1kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/647 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 647/647 [00:00<00:00, 994kB/s]\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: '%s' backend on %d nodes. Current host rank is %d. Number of gpus: %d gloo 2 1 4\u001b[0m\n",
      "\u001b[35mDownloading:   0%|          | 0.00/418M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35mDownloading:   1%|▏         | 5.32M/418M [00:00<00:07, 55.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   3%|▎         | 11.6M/418M [00:00<00:06, 61.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   4%|▍         | 17.9M/418M [00:00<00:06, 63.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   6%|▌         | 24.3M/418M [00:00<00:06, 65.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   7%|▋         | 30.9M/418M [00:00<00:06, 66.7MB/s]\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: '%s' backend on %d nodes. Current host rank is %d. Number of gpus: %d gloo 2 0 4\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/418M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 5.08M/418M [00:00<00:08, 52.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 10.9M/418M [00:00<00:07, 57.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 17.2M/418M [00:00<00:06, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 23.6M/418M [00:00<00:06, 63.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 29.9M/418M [00:00<00:06, 64.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:   9%|▉         | 37.6M/418M [00:00<00:05, 67.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  11%|█         | 44.2M/418M [00:00<00:05, 68.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  12%|█▏        | 50.9M/418M [00:00<00:05, 68.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  14%|█▍        | 57.5M/418M [00:00<00:05, 69.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  15%|█▌        | 64.2M/418M [00:01<00:05, 69.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  17%|█▋        | 70.8M/418M [00:01<00:05, 69.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  19%|█▊        | 77.5M/418M [00:01<00:05, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  20%|██        | 84.1M/418M [00:01<00:05, 69.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  22%|██▏       | 90.8M/418M [00:01<00:04, 69.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  23%|██▎       | 97.5M/418M [00:01<00:04, 69.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▊         | 36.3M/418M [00:00<00:06, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 42.7M/418M [00:00<00:05, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 49.1M/418M [00:00<00:05, 66.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 55.5M/418M [00:00<00:05, 66.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 61.9M/418M [00:01<00:05, 66.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 68.3M/418M [00:01<00:05, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 74.7M/418M [00:01<00:05, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 81.1M/418M [00:01<00:05, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 87.5M/418M [00:01<00:05, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 93.9M/418M [00:01<00:05, 67.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  25%|██▍       | 104M/418M [00:01<00:04, 69.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  27%|██▋       | 111M/418M [00:01<00:04, 69.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  28%|██▊       | 117M/418M [00:01<00:04, 69.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  30%|██▉       | 124M/418M [00:01<00:04, 69.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  31%|███▏      | 131M/418M [00:02<00:04, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  33%|███▎      | 137M/418M [00:02<00:04, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  34%|███▍      | 144M/418M [00:02<00:04, 69.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  36%|███▌      | 151M/418M [00:02<00:04, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  38%|███▊      | 157M/418M [00:02<00:03, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  39%|███▉      | 164M/418M [00:02<00:03, 69.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 100M/418M [00:01<00:04, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 107M/418M [00:01<00:04, 67.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 113M/418M [00:01<00:04, 66.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▊       | 119M/418M [00:01<00:04, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 126M/418M [00:02<00:04, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 132M/418M [00:02<00:04, 67.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 139M/418M [00:02<00:04, 67.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 145M/418M [00:02<00:04, 67.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 151M/418M [00:02<00:04, 67.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 158M/418M [00:02<00:04, 67.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  41%|████      | 171M/418M [00:02<00:03, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  42%|████▏     | 177M/418M [00:02<00:03, 69.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  44%|████▍     | 184M/418M [00:02<00:03, 69.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  46%|████▌     | 190M/418M [00:02<00:03, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  47%|████▋     | 197M/418M [00:03<00:03, 69.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  49%|████▉     | 204M/418M [00:03<00:03, 69.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  50%|█████     | 210M/418M [00:03<00:03, 69.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  52%|█████▏    | 217M/418M [00:03<00:03, 69.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  54%|█████▎    | 224M/418M [00:03<00:02, 69.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  55%|█████▌    | 230M/418M [00:03<00:02, 69.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 164M/418M [00:02<00:03, 67.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 171M/418M [00:02<00:05, 51.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 176M/418M [00:02<00:04, 51.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 182M/418M [00:03<00:04, 55.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 189M/418M [00:03<00:04, 57.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 195M/418M [00:03<00:03, 60.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 201M/418M [00:03<00:03, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 208M/418M [00:03<00:03, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 214M/418M [00:03<00:03, 62.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  57%|█████▋    | 237M/418M [00:03<00:02, 65.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  58%|█████▊    | 243M/418M [00:03<00:02, 65.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  60%|█████▉    | 250M/418M [00:03<00:02, 65.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  61%|██████    | 256M/418M [00:03<00:02, 65.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  63%|██████▎   | 262M/418M [00:04<00:02, 65.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  64%|██████▍   | 268M/418M [00:04<00:02, 65.9MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  66%|██████▌   | 275M/418M [00:04<00:02, 64.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  67%|██████▋   | 281M/418M [00:04<00:02, 64.8MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  69%|██████▊   | 287M/418M [00:04<00:02, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 220M/418M [00:03<00:03, 63.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 227M/418M [00:03<00:03, 64.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 233M/418M [00:03<00:02, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 240M/418M [00:03<00:02, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 246M/418M [00:04<00:02, 66.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 252M/418M [00:04<00:02, 66.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 259M/418M [00:04<00:02, 66.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 265M/418M [00:04<00:02, 66.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 272M/418M [00:04<00:02, 66.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 278M/418M [00:04<00:02, 67.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  70%|███████   | 293M/418M [00:04<00:02, 65.0MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  72%|███████▏  | 300M/418M [00:04<00:01, 65.1MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  73%|███████▎  | 306M/418M [00:04<00:01, 65.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  75%|███████▍  | 312M/418M [00:04<00:01, 65.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  76%|███████▌  | 318M/418M [00:04<00:01, 65.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  78%|███████▊  | 325M/418M [00:05<00:01, 65.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  79%|███████▉  | 331M/418M [00:05<00:01, 65.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  81%|████████  | 337M/418M [00:05<00:01, 65.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  82%|████████▏ | 343M/418M [00:05<00:01, 65.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  84%|████████▎ | 350M/418M [00:05<00:01, 65.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 285M/418M [00:04<00:02, 67.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 291M/418M [00:04<00:01, 67.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 297M/418M [00:04<00:01, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 304M/418M [00:04<00:01, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 310M/418M [00:05<00:01, 59.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 316M/418M [00:05<00:01, 61.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 322M/418M [00:05<00:01, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 328M/418M [00:05<00:01, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 335M/418M [00:05<00:01, 61.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  85%|████████▌ | 356M/418M [00:05<00:00, 65.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  87%|████████▋ | 362M/418M [00:05<00:00, 65.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  88%|████████▊ | 369M/418M [00:05<00:00, 65.4MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  90%|████████▉ | 375M/418M [00:05<00:00, 65.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  91%|█████████ | 381M/418M [00:05<00:00, 65.2MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  93%|█████████▎| 387M/418M [00:06<00:00, 65.3MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  94%|█████████▍| 394M/418M [00:06<00:00, 65.5MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  96%|█████████▌| 400M/418M [00:06<00:00, 65.7MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  97%|█████████▋| 406M/418M [00:06<00:00, 65.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading:  99%|█████████▊| 412M/418M [00:06<00:00, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 341M/418M [00:05<00:01, 62.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 348M/418M [00:05<00:01, 64.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 354M/418M [00:05<00:01, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 360M/418M [00:05<00:01, 57.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 366M/418M [00:06<00:00, 59.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 372M/418M [00:06<00:00, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 379M/418M [00:06<00:00, 62.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 385M/418M [00:06<00:00, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▎| 391M/418M [00:06<00:00, 64.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 398M/418M [00:06<00:00, 65.6MB/s]\u001b[0m\n",
      "\u001b[35mDownloading: 100%|██████████| 418M/418M [00:06<00:00, 67.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 404M/418M [00:06<00:00, 65.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 411M/418M [00:06<00:00, 66.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 417M/418M [00:06<00:00, 66.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 418M/418M [00:06<00:00, 64.0MB/s]\u001b[0m\n",
      "\u001b[35mSome weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[35m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[35m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m=============== EPOCH 1 / 4 ===============\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m=============== EPOCH 1 / 4 ===============\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.134 algo-1:41 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.244 algo-1:41 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.244 algo-1:41 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.245 algo-1:41 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.245 algo-1:41 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.245 algo-1:41 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:22881792\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.487 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.488 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.489 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.490 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.491 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.492 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.493 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.494 algo-1:41 INFO hook.py:591] name:module.bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.495 algo-1:41 INFO hook.py:591] name:module.bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.495 algo-1:41 INFO hook.py:591] name:module.out.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.495 algo-1:41 INFO hook.py:591] name:module.out.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.495 algo-1:41 INFO hook.py:593] Total Trainable Params: 108924674\u001b[0m\n",
      "\u001b[34m[2022-06-27 01:36:56.495 algo-1:41 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.021 algo-2:40 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.129 algo-2:40 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.130 algo-2:40 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.130 algo-2:40 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.131 algo-2:40 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.131 algo-2:40 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.385 algo-2:40 INFO hook.py:591] name:module.bert.embeddings.word_embeddings.weight count_params:22881792\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.385 algo-2:40 INFO hook.py:591] name:module.bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.385 algo-2:40 INFO hook.py:591] name:module.bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.385 algo-2:40 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.385 algo-2:40 INFO hook.py:591] name:module.bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.385 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.386 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.387 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.388 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.389 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.390 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.391 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.392 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.393 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.394 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.395 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.396 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.397 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.398 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.399 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.400 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.out.weight count_params:1536\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:591] name:module.out.bias count_params:2\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.401 algo-2:40 INFO hook.py:593] Total Trainable Params: 108924674\u001b[0m\n",
      "\u001b[35m[2022-06-27 01:36:59.402 algo-2:40 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m___ batch index = 0 / 4957 (0.00%), loss = 0.7337, time = 6.64 seconds ___\u001b[0m\n",
      "\u001b[35mINFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m___ batch index = 0 / 4957 (0.00%), loss = 0.6902, time = 6.64 seconds ___\u001b[0m\n",
      "\u001b[34mINFO:root:Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[35m___ batch index = 100 / 4957 (2.02%), loss = 0.3359, time = 107.13 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 100 / 4957 (2.02%), loss = 0.2603, time = 107.13 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 200 / 4957 (4.03%), loss = 0.2336, time = 104.69 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 200 / 4957 (4.03%), loss = 0.2833, time = 104.69 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 300 / 4957 (6.05%), loss = 0.1595, time = 106.76 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 300 / 4957 (6.05%), loss = 0.1954, time = 106.76 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 400 / 4957 (8.07%), loss = 0.2488, time = 106.79 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 400 / 4957 (8.07%), loss = 0.2545, time = 106.79 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 500 / 4957 (10.09%), loss = 0.2719, time = 105.86 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 500 / 4957 (10.09%), loss = 0.1684, time = 105.86 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 600 / 4957 (12.10%), loss = 0.2160, time = 107.55 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 600 / 4957 (12.10%), loss = 0.2318, time = 107.55 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 700 / 4957 (14.12%), loss = 0.2083, time = 105.39 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 700 / 4957 (14.12%), loss = 0.1664, time = 105.39 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 800 / 4957 (16.14%), loss = 0.1666, time = 105.40 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 800 / 4957 (16.14%), loss = 0.1489, time = 105.40 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 900 / 4957 (18.16%), loss = 0.2333, time = 106.07 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 900 / 4957 (18.16%), loss = 0.1936, time = 106.07 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1000 / 4957 (20.17%), loss = 0.2018, time = 108.35 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1000 / 4957 (20.17%), loss = 0.1641, time = 108.35 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1100 / 4957 (22.19%), loss = 0.1688, time = 105.65 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1100 / 4957 (22.19%), loss = 0.1003, time = 105.65 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1200 / 4957 (24.21%), loss = 0.2479, time = 110.82 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1200 / 4957 (24.21%), loss = 0.2573, time = 110.82 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1300 / 4957 (26.23%), loss = 0.1683, time = 104.41 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1300 / 4957 (26.23%), loss = 0.1844, time = 104.41 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1400 / 4957 (28.24%), loss = 0.1181, time = 105.37 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1400 / 4957 (28.24%), loss = 0.2153, time = 105.37 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1500 / 4957 (30.26%), loss = 0.1650, time = 105.49 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1500 / 4957 (30.26%), loss = 0.1404, time = 105.49 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1600 / 4957 (32.28%), loss = 0.1526, time = 105.78 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1600 / 4957 (32.28%), loss = 0.1681, time = 105.78 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1700 / 4957 (34.29%), loss = 0.1327, time = 106.52 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1700 / 4957 (34.29%), loss = 0.1370, time = 106.52 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1800 / 4957 (36.31%), loss = 0.1529, time = 104.66 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1800 / 4957 (36.31%), loss = 0.1830, time = 104.66 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 1900 / 4957 (38.33%), loss = 0.1430, time = 105.28 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 1900 / 4957 (38.33%), loss = 0.1047, time = 105.28 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 2000 / 4957 (40.35%), loss = 0.1380, time = 106.96 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 2000 / 4957 (40.35%), loss = 0.1014, time = 106.96 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 2100 / 4957 (42.36%), loss = 0.1419, time = 105.39 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 2100 / 4957 (42.36%), loss = 0.1531, time = 105.38 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 2200 / 4957 (44.38%), loss = 0.1764, time = 108.84 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 2200 / 4957 (44.38%), loss = 0.1757, time = 108.84 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 2300 / 4957 (46.40%), loss = 0.1315, time = 106.17 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 2300 / 4957 (46.40%), loss = 0.1126, time = 106.17 seconds ___\u001b[0m\n",
      "\u001b[35m___ batch index = 2400 / 4957 (48.42%), loss = 0.1163, time = 104.96 seconds ___\u001b[0m\n",
      "\u001b[34m___ batch index = 2400 / 4957 (48.42%), loss = 0.2420, time = 104.96 seconds ___\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# place to save model artifact\n",
    "output_path = f\"s3://{bucket}/{model_prefix}\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"script\",\n",
    "    role=role,\n",
    "    framework_version=\"1.10.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=2,  # this script only support distributed training for GPU instances.\n",
    "    instance_type=\"ml.p3.8xlarge\",\n",
    "    # instance_type=\"local\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"batch-size\": 16,\n",
    "        \"epochs\": 3,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    },\n",
    "    disable_profiler=True, # disable debugger\n",
    ")\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"lr\": CategoricalParameter([3e-4, 1e-4, 5e-5, 3e-5]),\n",
    "    \"batch-size\": CategoricalParameter([4, 8, 16]),\n",
    "}\n",
    "\n",
    "# change to accuracy\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=8,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type=objective_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\"training\": inputs})"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
