{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install ipywidgets\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, classification_report, plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BERT_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
    "# BERT_MODEL_NAME = 'neuralmind/bert-large-portuguese-cased'\n",
    "\n",
    "\n",
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationModel, self).__init__()\n",
    "        self.bert_path = BERT_MODEL_NAME\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        self.bert_drop=nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 2)\n",
    "        # self.out = nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        outputs = self.bert(input_ids=ids, attention_mask=mask)\n",
    "        rh=self.bert_drop(outputs.pooler_output)\n",
    "        return self.out(rh)\n",
    "\n",
    "\n",
    "class CudaDevice:\n",
    "    def get_device(self, force_cpu=False):\n",
    "        if not force_cpu and torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print('Existe(m) %d GPU(s) disponíveis.' %\n",
    "                  torch.cuda.device_count())\n",
    "            print('A GPU', torch.cuda.get_device_name(0), 'será usada.')\n",
    "        else:\n",
    "            print('No GPU available, using the CPU instead.')\n",
    "            device = torch.device(\"cpu\")\n",
    "        return device\n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 tokenizer,\n",
    "                 max_len=None,\n",
    "                 max_size_dataset=None,\n",
    "                 ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_raw = data\n",
    "        self.max_len = max_len\n",
    "        self.max_size_dataset = max_size_dataset\n",
    "        self.data, self.label = self.process_data()\n",
    "        \n",
    "\n",
    "    def process_data(self):\n",
    "        \n",
    "        print('Carregando os dados do dataset')\n",
    "        df = self.data_raw.copy()\n",
    "\n",
    "        if self.max_len:\n",
    "            print(f'Limitando textos em {self.max_len} caracteres!')\n",
    "            df['text'] = df['text'].apply(lambda x: x[:self.max_len] if len(x) > self.max_len else x)\n",
    "\n",
    "        train = df.copy()\n",
    "\n",
    "        if(self.max_size_dataset):\n",
    "            print(f'Limitando dataset em {self.max_size_dataset} registros!')\n",
    "            train = train.loc[0:self.max_size_dataset, :]\n",
    "            \n",
    "        train = train.reindex(np.random.permutation(train.index))\n",
    "        train['text'] = train['text'].apply(self.clean_txt)\n",
    "        return train['text'].values, train['label'].values\n",
    "\n",
    "    def clean_txt(self, text):\n",
    "        # text = re.sub(\"'\", \"\", text)\n",
    "        # text = re.sub(\"(\\\\W)+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        text = str(self.data[idx])\n",
    "        target = int(self.label[idx])\n",
    "\n",
    "        data = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_overflowing_tokens=False,\n",
    "            truncation=True,\n",
    "            return_tensors='pt')\n",
    "\n",
    "        return({\n",
    "            'ids': data['input_ids'].long(),\n",
    "            'mask': data['attention_mask'].int(),\n",
    "            'targets': torch.tensor([target], dtype=torch.int),\n",
    "            'texto': text.strip(),\n",
    "        })\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\" Return data length \"\"\"\n",
    "        return self.label.shape[0]\n",
    "\n",
    "\n",
    "def my_collate1(batches):\n",
    "    return [{key: value for key, value in batch.items()} for batch in batches]\n",
    "\n",
    "\n",
    "def loss_fun(outputs, targets):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(outputs, targets)\n",
    "\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    \n",
    "    true_label_mask = [1 if (np.argmax(x)-target[i]) ==\n",
    "                       0 else 0 for i, x in enumerate(predicted)]\n",
    "    nb_prediction = len(true_label_mask)\n",
    "    true_prediction = sum(true_label_mask)\n",
    "    false_prediction = nb_prediction-true_prediction\n",
    "    accuracy = true_prediction/nb_prediction\n",
    "    \n",
    "    roc = roc_auc_score(target, predicted[:, 1])\n",
    "    \n",
    "    return{\n",
    "        \"accuracy\": accuracy,\n",
    "        \"nb examples\": len(target),\n",
    "        \"true_prediction\": true_prediction,\n",
    "        \"false_prediction\": false_prediction,\n",
    "        \"roc_auc\": roc\n",
    "    }\n",
    "\n",
    "\n",
    "def train_loop(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        ids = [data[\"ids\"] for data in batch]\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch]\n",
    "\n",
    "        ids = torch.cat(ids)\n",
    "        mask = torch.cat(mask)\n",
    "        targets = torch.cat(targets)\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids=ids, mask=mask)\n",
    "        loss = loss_fun(outputs, targets)\n",
    "        loss.backward()\n",
    "        model.float()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"___ batch index = {batch_idx} / {len(data_loader)} ({100*batch_idx / len(data_loader):.2f}%), loss = {np.mean(losses[-10:]):.4f}, time = {time.time()-t0:.2f} seconds ___\")\n",
    "            t0 = time.time()\n",
    "    return losses    \n",
    "\n",
    "\n",
    "def eval_loop(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in tqdm.tqdm(enumerate(data_loader)):\n",
    "        ids = [data[\"ids\"] for data in batch]\n",
    "        mask = [data[\"mask\"] for data in batch]\n",
    "        targets = [data[\"targets\"] for data in batch]\n",
    "\n",
    "        ids = torch.cat(ids)\n",
    "        mask = torch.cat(mask)\n",
    "        targets = torch.cat(targets)\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids=ids, mask=mask)\n",
    "            loss = loss_fun(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        fin_targets.append(targets.cpu().detach().numpy())\n",
    "        fin_outputs.append(torch.softmax(outputs, dim=1).cpu().detach().numpy())\n",
    "\n",
    "    return np.concatenate(fin_outputs), np.concatenate(fin_targets), losses    \n",
    "\n",
    "########################################################################################\n",
    "# variáveis globais\n",
    "########################################################################################\n",
    "device = CudaDevice().get_device(force_cpu=False)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EPOCH = 3\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "MAX_LEN = None\n",
    "MAX_SIZE_DATASET = None\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# dataset\n",
    "########################################################################################\n",
    "train_raw = pd.read_csv('https://docs.google.com/uc?export=download&id=1_EKfnjomkWks4VqTMIpcEIb6nB5P0Xz2')\n",
    "train_raw.columns = ['label','text']\n",
    "train_raw['label'] = train_raw['label'].apply(lambda x: 1 if x == 'positivo' else 0)\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# se quiser trabalhar com amostra menor\n",
    "########################################################################################\n",
    "SAMPLE_SIZE = 1000\n",
    "s_labels = train_raw['label'].value_counts(normalize=True).sort_index()\n",
    "train_raw = pd.concat([\n",
    "    train_raw[train_raw['label']==0].sample(int(SAMPLE_SIZE * s_labels[0]), random_state=random_seed), # ~0.427427\n",
    "    train_raw[train_raw['label']==1].sample(int(SAMPLE_SIZE * s_labels[1]), random_state=random_seed), # ~0.572573\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Carregando os dados do dataset\n",
      "Carregando os dados do dataset\n",
      "Carregando os dados do dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 140 (0.00%), loss = 0.6810, time = 0.40 seconds ___\n",
      "___ batch index = 100 / 140 (71.43%), loss = 0.0501, time = 39.28 seconds ___\n",
      "\n",
      "*** avg_loss : 0.23, time : ~0.0 min (55.52 sec) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:05,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> evaluation : avg_loss = 0.17, time : 5.47 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.9214285714285714, 'nb examples': 140, 'true_prediction': 129, 'false_prediction': 11, 'roc_auc': 0.9883333333333335}\n",
      "\n",
      "=============== EPOCH 2 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 140 (0.00%), loss = 0.0179, time = 0.45 seconds ___\n",
      "___ batch index = 100 / 140 (71.43%), loss = 0.0078, time = 41.63 seconds ___\n",
      "\n",
      "*** avg_loss : 0.07, time : ~0.0 min (58.23 sec) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:05,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> evaluation : avg_loss = 0.18, time : 5.55 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.95, 'nb examples': 140, 'true_prediction': 133, 'false_prediction': 7, 'roc_auc': 0.9860416666666667}\n",
      "\n",
      "=============== EPOCH 3 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 140 (0.00%), loss = 0.0052, time = 0.47 seconds ___\n",
      "___ batch index = 100 / 140 (71.43%), loss = 0.0058, time = 41.68 seconds ___\n",
      "\n",
      "*** avg_loss : 0.03, time : ~0.0 min (58.34 sec) ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:05,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> evaluation : avg_loss = 0.15, time : 5.54 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.9642857142857143, 'nb examples': 140, 'true_prediction': 135, 'false_prediction': 5, 'roc_auc': 0.9841666666666666}\n"
     ]
    }
   ],
   "source": [
    "print('Loading BERT tokenizer...')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_raw[['text']], \n",
    "    train_raw.label, \n",
    "    test_size=0.3, \n",
    "    stratify=train_raw.label,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train[['text']], \n",
    "    y_train, \n",
    "    test_size=0.2, \n",
    "    stratify=y_train,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "train_dataset = BertDataset(\n",
    "    data=pd.concat([X_train, y_train], axis=1),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    max_size_dataset=MAX_SIZE_DATASET,\n",
    ")\n",
    "\n",
    "val_dataset = BertDataset(\n",
    "    data=pd.concat([X_val, y_val], axis=1),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    max_size_dataset=MAX_SIZE_DATASET,\n",
    ")\n",
    "\n",
    "test_dataset = BertDataset(\n",
    "    data=pd.concat([X_test, y_test], axis=1),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN,\n",
    "    max_size_dataset=MAX_SIZE_DATASET,\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=TRAIN_BATCH_SIZE,\n",
    "  collate_fn=my_collate1\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=TRAIN_BATCH_SIZE,\n",
    "  collate_fn=my_collate1\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=TRAIN_BATCH_SIZE,\n",
    "  collate_fn=my_collate1\n",
    ")\n",
    "\n",
    "lr=3e-5\n",
    "num_training_steps=int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=BertClassificationModel().to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "batches_probs=[]\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp = eval_loop(valid_data_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(np.mean(val_losses_tmp))\n",
    "    batches_losses.append(np.mean(batches_losses_tmp))\n",
    "    batches_probs.append(output)\n",
    "    torch.save(model, f\"./checkpoints/model_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [00:10,  7.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./checkpoints/model_epoch_2.pt\")\n",
    "output, target, val_losses_tmp = eval_loop(test_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> evaluation : avg_loss = 0.20 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.9466666666666667, 'nb examples': 300, 'true_prediction': 284, 'false_prediction': 16, 'roc_auc': 0.9828760901162791}\n"
     ]
    }
   ],
   "source": [
    "print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f} sec\\n\")\n",
    "tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "print(f\"=====>\\t{tmp_evaluate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94       128\n",
      "           1       0.96      0.94      0.95       172\n",
      "\n",
      "    accuracy                           0.95       300\n",
      "   macro avg       0.94      0.95      0.95       300\n",
      "weighted avg       0.95      0.95      0.95       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = [1 if ot > .5 else 0 for ot in output[:, 1]]\n",
    "print(classification_report(target, o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(normalize='true', y_pred=o, y_true=target)\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe(m) 4 GPU(s) disponíveis.\n",
      "A GPU Tesla T4 será usada.\n",
      "Predição = Neg(0.0893), Pos(0.9107)\n"
     ]
    }
   ],
   "source": [
    "device = CudaDevice().get_device(force_cpu=False)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "model = torch.load(\"./checkpoints/model_epoch_2.pt\")\n",
    "# Verifica a probabilidade da predição\n",
    "novo_texto = 'Não vi outro produto que atende tão bem a minha necessidade.'\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  data = tokenizer.encode_plus(\n",
    "    novo_texto,\n",
    "    max_length=512,\n",
    "    pad_to_max_length=True,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_overflowing_tokens=False,\n",
    "    truncation=True,\n",
    "    return_tensors='pt')\n",
    "  \n",
    "  ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "  mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "  otps = model(ids=ids, mask=mask)\n",
    "  pred_proba = torch.softmax(otps, dim=1)\n",
    "\n",
    "print(f'Predição = Neg({pred_proba[0][0]:0.4f}), Pos({pred_proba[0][1]:0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Não vi outro produto que atende tão bem a minha necessidade.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd = tokenizer.encode_plus(\n",
    "    novo_texto,\n",
    "    max_length=512,\n",
    "    pad_to_max_length=True,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_overflowing_tokens=False,\n",
    "    truncation=True,\n",
    "    return_tensors='pt')\n",
    "\n",
    "# tokenizer.convert_ids_to_tokens(ddd['input_ids'][0, :])\n",
    "\n",
    "ddd['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predição = Neg(0.0189), Pos(0.9811)\n"
     ]
    }
   ],
   "source": [
    "novo_texto = 'Parabéns, você é excelente em fazer besteira.'\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  data = tokenizer.encode_plus(\n",
    "    novo_texto,\n",
    "    max_length=512,\n",
    "    pad_to_max_length=True,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_overflowing_tokens=False,\n",
    "    truncation=True,\n",
    "    return_tensors='pt')\n",
    "  \n",
    "  ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "  mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "  otps = model(ids=ids, mask=mask)\n",
    "  pred_proba = torch.softmax(otps, dim=1)\n",
    "\n",
    "print(f'Predição = Neg({pred_proba[0][0]:0.4f}), Pos({pred_proba[0][1]:0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Para',\n",
       " '##b',\n",
       " '##éns',\n",
       " ',',\n",
       " 'você',\n",
       " 'é',\n",
       " 'excelente',\n",
       " 'em',\n",
       " 'fazer',\n",
       " 'be',\n",
       " '##st',\n",
       " '##eira',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids[0])[:20]"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
